{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information maximiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using neural networks, sufficient statistics can be obtained from data by maximising the Fisher information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this code please cite <a href=\"https://arxiv.org/abs/1802.03537\">arXiv:1802.03537</a>.<br><br>\n",
    "The code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:<br><br>\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1175196.svg)](https://doi.org/10.5281/zenodo.1175196)\n",
    "<br>\n",
    "The code presented below is version two (and is much more powerful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is run using<br>\n",
    ">`python-3.6.1`\n",
    "\n",
    ">`jupyter-1.0.0`\n",
    "\n",
    ">`tensorflow-1.8.0`\n",
    "\n",
    ">`numpy-1.14.2`\n",
    "\n",
    ">`tqdm==4.19.9`\n",
    "\n",
    ">`sys (native)`\n",
    "\n",
    "Although these precise versions may not be necessary, I have put them here to avoid possible conflicts. For reference, all code is run on a nVidia GeForce GTX 1080Ti (OC Strix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:02.400626Z",
     "start_time": "2018-08-02T13:47:58.710103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charnock/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IMNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliase the neural network\n",
    "### Define network parameters\n",
    "The network works with a base set of parameters which are<br>\n",
    "> `'verbose'` - `bool` - whether to print out diagnostics\n",
    "\n",
    "> `'number of simulations'` - `int` - the number of simulations to use in any one combination\n",
    "\n",
    "> `'differentiation fraction'` - `float` - a fraction of the simulations to use for the numerical derivative\n",
    "\n",
    "> `'number of parameters'` - `int` - number of parameters in a model\n",
    "\n",
    "> `'number of summaries'` - `int` - number of summaries the network makes from the data\n",
    "\n",
    "> `'input shape'` - `int` or `list` - the number of inputs or the shape of the input if image-like input\n",
    "\n",
    "> `'prebuild'` - `bool` - whether to get the network to build a network or to provided your own\n",
    "\n",
    "> `'save file'` - `string` - a file name to save the graph (not saved if wrong type or not given)\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': 1000,\n",
    "    'differentiation fraction': .1,\n",
    "    'number of parameters': 1,\n",
    "    'number of summaries': 1,\n",
    "    'input shape': [10, 20, 1],\n",
    "    'prebuild': False,\n",
    "}\n",
    "```\n",
    "The module can also build simple convolutional or dense networks (or a mixture of the two), which can be trigger by setting `'prebuild': True`. Several parameters are required to allow the network to build the network. These are<br>\n",
    "> `'wv'` - `float` - the variance with which to initialise the weights. If this is 0 or less, the network will determine the weight variance\n",
    "\n",
    "> `'bb'` - `float` - the constant value with which to initialise the biases\n",
    "\n",
    "> `'activation'` - `TensorFlow function` - a native tensorflow activation function\n",
    "\n",
    "> `'α'` - `float` or `int` - an additional parameter, if needed, for the tensorflow activation function\n",
    "\n",
    "> `'hidden layers'` - `list` - the architecture of the network. each element of the list is a hidden layer. A dense layer can be made using an integer where thet value indicates the number of neurons. A convolutional layer can be built by using a list where the first element is an integer where the number describes the number of filters, the second element is a list of the kernel size in the x and y directions, the third elemnet is a list of the strides in the x and y directions and the final element is string of 'SAME' or 'VALID' which describes the padding prescription.\n",
    "\n",
    "Here is an example of the IMNN which uses 1000 simulations per combination and 100 simulations per derivative for a model with one parameter where we require one summary. The module will build the network which takes in an input image of shape `[10, 20, 1]` and allows the network to decide the weight initialisation, initialises the biases at `bb = 0.1` and uses `tf.nn.leaky_relu` activation with a negative gradient parameter of `α = 0.01`. The network architecture is a convolution with 10 filters and a 5$\\times$5 kernel which does 2$\\times$2 strides, with 0-padding, followed by another convolution with 6 filters and 3$\\times$3 kernel with no striding and 0-padding. This is then followed by two dense layers with 100 neurons in each. We will save the graph into a file in the `data` folder called `saved_model.meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:02.420404Z",
     "start_time": "2018-08-02T13:48:02.406279Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': 1000,\n",
    "    'number of parameters': 1,\n",
    "    'differentiation fraction': .1,\n",
    "    'number of summaries': 1,\n",
    "    'save file': \"data/saved_model\",\n",
    "    'prebuild': True,\n",
    "    'input shape': [10, 20, 1],\n",
    "    'wv': 0.,\n",
    "    'bb': 0.1,\n",
    "    'activation': tf.nn.leaky_relu,\n",
    "    'α': 0.01,\n",
    "    'hidden layers': [[10, [5, 5], [2, 2], 'SAME'], [6, [3, 3], [1, 1], 'SAME'], 100, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the network parameters are initialised using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:02.442764Z",
     "start_time": "2018-08-02T13:48:02.427410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model as data/saved_model.meta\n",
      "network architecture is [[10, 20, 1], [10, [5, 5], [2, 2], 'SAME'], [6, [3, 3], [1, 1], 'SAME'], 100, 100, 1].\n"
     ]
    }
   ],
   "source": [
    "n = IMNN.IMNN(parameters = parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "To build the network a learning rate, η, must be defined. The `setup(η)` function initialises the input tensors, builds the network and defines the optimisation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:06.731651Z",
     "start_time": "2018-08-02T13:48:02.447044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN/layer_1/conv_1/mul:0\", shape=(?, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN/layer_2/conv_2/mul:0\", shape=(?, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN/layer_3/dense_3/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN/layer_4/dense_4/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN/layer_5/LeakyRelu/Maximum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"StopGradient:0\", shape=(?, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1/conv_1/mul:0\", shape=(?, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2/conv_2/mul:0\", shape=(?, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3/dense_3/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4/dense_4/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5/LeakyRelu/Maximum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"StopGradient_1:0\", shape=(?, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_1/conv_1/mul:0\", shape=(?, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_1/conv_2/mul:0\", shape=(?, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_1/dense_3/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_1/dense_4/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_1/LeakyRelu/Maximum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"central_output:0\", shape=(?, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_mean:0\", shape=(?, 1, 1), dtype=float32)\n",
      "Tensor(\"central_difference_from_mean:0\", shape=(?, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_covariance:0\", shape=(?, 1, 1), dtype=float32)\n",
      "Tensor(\"central_inverse_covariance:0\", shape=(?, 1, 1), dtype=float32)\n",
      "Tensor(\"lower_output:0\", shape=(?, 100, 1, 1), dtype=float32)\n",
      "Tensor(\"upper_output:0\", shape=(?, 100, 1, 1), dtype=float32)\n",
      "Tensor(\"mean_derivative:0\", shape=(?, 1, 1), dtype=float32)\n",
      "Tensor(\"fisher_information:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n.setup(η = 1e-4, network = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-defined network\n",
    "A self defined network can be used instead of letting the module build the network for you. To do this you simply pass a function which contains the network to `setup(η = η, network = network)`. This function needs to take in two input tensors, the first is the shape of the input with `None` in the first axis and the second tensor is a tensorflow float (which will be the dropout). Since the weights need to be shared between several corresponding networks each set of trainable variables must be defined in its own scope. An example of the above network defined outside of the module is\n",
    "```python\n",
    "def network(input_tensor, dropout):\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        weights = tf.get_variable(\"weights\", [5, 5, 1, 10], initializer = tf.random_normal_initializer(0., 1.))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.nn.conv2d(input_tensor, weights, [1, 2, 2, 1], padding = 'SAME')\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_2'):\n",
    "        weights = tf.get_variable(\"weights\", [3, 3, 10, 6], initializer = tf.random_normal_initializer(0., 1.))\n",
    "        biases = tf.get_variable(\"biases\", [6], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding = 'SAME')\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    x = tf.reshape(x, (-1, 300))\n",
    "    with tf.variable_scope('layer_3'):\n",
    "        weights = tf.get_variable(\"weights\", [300, 100], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 300)))\n",
    "        biases = tf.get_variable(\"biases\", [100], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_4'):\n",
    "        weights = tf.get_variable(\"weights\", [100, 100], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 100)))\n",
    "        biases = tf.get_variable(\"biases\", [100], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_5'):\n",
    "        weights = tf.get_variable(\"weights\", [100, 1], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 100)))\n",
    "        biases = tf.get_variable(\"biases\", [1], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing minimisation scheme\n",
    "By default the optimation scheme is<br>\n",
    "```python\n",
    "n.backpropagate = tf.train.GradientDescentOptimizer(η).minimize(Λ)\n",
    "```\n",
    "where Λ is the loss function tensor. To use any other training scheme, such as the `Adam` optimiser, it is sufficient to run\n",
    "```python\n",
    "n.backpropagate = tf.train.AdamOptimizer(η, β1, β2, ε).minimize(n.loss(n.F))\n",
    "```\n",
    "after `setup(η)` to override the default minimisation routine. If you want to continue to use the default minimisation routine but want to change the learning rate without reinitialising you can run \n",
    "```python\n",
    "n.training_scheme(η = new_η)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "Here were going to use a 2D field of Gaussian noise with zero mean and unknown variance to see if the network can learn to summarise this variance.<br><br>\n",
    "We start by defining a function to generate the data with the correct shape. This should be \n",
    "```\n",
    "data_shape = None + input shape\n",
    "```\n",
    "It is useful to define this function so that it only takes in the value of the parameter as its input since the function can then be used for ABC later.<br><br>\n",
    "The data needs to be generated at a fiducial parameter value and at perturbed values just below and above the fiducial parameter for the numerical derivative. The data at the perturbed values should have the shape\n",
    "```\n",
    "perturbed_data_shape = None + number of parameters + input shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:06.747955Z",
     "start_time": "2018-08-02T13:48:06.735708Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(θ, train = False):\n",
    "    if train:\n",
    "        return np.moveaxis(np.random.normal(0., np.sqrt(θ), [1] + n.inputs + [len(θ)]), -1, 0)\n",
    "    else:\n",
    "        return np.moveaxis(np.random.normal(0., np.sqrt(θ), n.inputs + [len(θ)]), -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "Enough data needs to be made to train the network sucessfully. This normally means several combinations of large amounts of simulations. We have already chosen the number of simulations which makes up a single combination. This is stored in the parameter `n.n_s` and in our case `n.n_s = 1000` and we use all of the simulations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:06.758254Z",
     "start_time": "2018-08-02T13:48:06.751614Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fiducial parameter data can now be created, with variance θ = 1. We define how many simulations to use by passing the fiducial parameter through as a list. This is very useful for the ABC function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:06.786330Z",
     "start_time": "2018-08-02T13:48:06.761197Z"
    }
   },
   "outputs": [],
   "source": [
    "t = generate_data(θ = [1. for i in range(n_train * n.n_s)], train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential to suppress the sample variance between the lower and upper perturbed values which can be done by setting the seed to be equal before calling the data generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:06.815997Z",
     "start_time": "2018-08-02T13:48:06.791349Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = np.random.randint(1e6)\n",
    "np.random.seed(seed)\n",
    "t_m = generate_data(θ = [0.9 for i in range(n_train * n.n_p)], train = True)\n",
    "np.random.seed(seed)\n",
    "t_p = generate_data(θ = [1.1 for i in range(n_train * n.n_p)], train = True)\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the denominator of the derivative which is given by the difference between the perturbed parameter values<br><br>\n",
    "$$\\frac{\\partial}{\\partial\\theta} = \\frac{1}{1.1 - 0.9}.$$<br>\n",
    "This needs to be done for every parameter and kept in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:09.354275Z",
     "start_time": "2018-08-02T13:48:09.348126Z"
    }
   },
   "outputs": [],
   "source": [
    "derivative_denominator = 1. / 0.2\n",
    "der_den = np.array([derivative_denominator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the data needs collecting in a list to pass to the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:09.908996Z",
     "start_time": "2018-08-02T13:48:09.902764Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = [t, t_m, t_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "We also need to make some test data, although we don't need multiple combinations of this, i.e. `n_train = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:10.335723Z",
     "start_time": "2018-08-02T13:48:10.265030Z"
    }
   },
   "outputs": [],
   "source": [
    "tt = generate_data([1. for i in range(n.n_s)])\n",
    "seed = np.random.randint(1e6)\n",
    "np.random.seed(seed)\n",
    "tt_m = generate_data([0.9 for i in range(n.n_p)], train = True)\n",
    "np.random.seed(seed)\n",
    "tt_p = generate_data([1.1 for i in range(n.n_p)], train = True)\n",
    "np.random.seed()\n",
    "test_data = [tt, tt_m, tt_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation\n",
    "We can simply plot a projection of some of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:48:11.937998Z",
     "start_time": "2018-08-02T13:48:11.046112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123fa7b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (20, 12))\n",
    "plt.subplots_adjust(wspace = 0)\n",
    "ax[0].imshow(t[np.random.randint(n_train * n.n_s), :, :, 0])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_xlabel('Training image')\n",
    "ax[1].imshow(tt[np.random.randint(n.n_s), :, :, 0])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('Test image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "With the data we can now easily train the network. The train function takes in the training data (and the test data if you want to test - `test_data = None` can be used if you don't want to test, in which case the function only returns one list).<br><br>\n",
    "As well as the data, the function needs the number of epochs, `num_epochs`, the number of combinations of data, `n_train`, the number of batches, `num_batches < num_epochs`, the fraction of neurons kept when using dropout `keep_rate`, and the denominator for the derivative calculated earlier, `der_den`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:48:17.245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 163/1500 [01:38<13:24,  1.66it/s, detF=0.408, detF_test=0.2]   "
     ]
    }
   ],
   "source": [
    "train_F, test_F = n.train(train_data = train_data, num_epochs = 1500, n_train = n_train, num_batches = 1, keep_rate = 0.8, der_den = der_den, test_data = test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the the train function is a list of the determinant of the Fisher information at the end of each epoch of training on the train data, and the same on the test data if test data is provided.<br><br>\n",
    "These can be plotted, along with the loss function which is simply\n",
    "$$\\Lambda = -\\frac{1}{2}|F|^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:48:18.564Z"
    }
   },
   "outputs": [],
   "source": [
    "train_F = np.array(train_F)\n",
    "test_F = np.array(test_F)\n",
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 14))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "end = len(train_F)\n",
    "epochs = np.arange(end)\n",
    "a, = ax[0].plot(epochs, -0.5 * train_F[:end]**2, label = 'Training data')\n",
    "b, = ax[0].plot(epochs, -test_F[:end]**2, label = 'Test data')\n",
    "ax[0].legend(frameon = False)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(epochs, train_F[:end])\n",
    "ax[1].plot(epochs, test_F[:end])\n",
    "ax[1].set_ylabel('$|\\mathcal{F}|$')\n",
    "ax[1].set_xlabel('Number of epochs')\n",
    "ax[1].set_xlim([0, len(epochs)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test loss deviates from the training loss. This is to be expected because there are will be a lot of correlation within a small training set which isn't in the test set. As long as the test loss doesn't start increasing then it is likely that the network is still working, with the maximum Fisher available being the value obtained from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the network\n",
    "If you need to reset the weights and biases (for example if the covariance becomes zero and the weights become `NaN`) then you can call\n",
    "```python\n",
    "n.reinitialise_session()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the network\n",
    "\n",
    "If you don't initialise the network with a save name you can save the network as a `TensorFlow` `meta` graph. For example saving the model in the directory `/.data` called `saved_model.meta` can be done using the function\n",
    "```python\n",
    "n.save_file = \"data/saved_model\"\n",
    "n.save_network(first_time = True)\n",
    "```\n",
    "If `save file` is passed with a correct file name when initialising the module then the initialised network will be saved by\n",
    "```python\n",
    "n.begin_session()\n",
    "```\n",
    "and then saved at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the network\n",
    "\n",
    "You can load the network from a `TensorFlow` `meta` graph (from `/.data/saved_model.meta`) either in a preinitialised module using\n",
    "```python\n",
    "tf.reset_default_graph()\n",
    "n.save_file = \"data/saved_model\"\n",
    "n.restore_network()\n",
    "```\n",
    "or more simply, initialise the module using\n",
    "```python\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': 500,\n",
    "    'differentiation fraction': .1,\n",
    "    'number of parameters': 1,\n",
    "    'number of summaries': 1,\n",
    "    'input shape': [10, 20, 1],\n",
    "    'prebuild': False,\n",
    "    'save file': \"data/saved_model\",\n",
    "}\n",
    "```\n",
    "and then running\n",
    "```python\n",
    "n = IMNN.IMNN(parameters = parameters)\n",
    "n.restore_network()\n",
    "```\n",
    "Training can be continued after restoring the model - although the Adam optimiser might need to reacquaint itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Bayesian computation\n",
    "We can now do ABC (or PMC-ABC) with our calculated summary. First we generate some simulated real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:33.000Z"
    }
   },
   "outputs": [],
   "source": [
    "real_data = generate_data(θ = [1.], train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this real data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:33.915Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 12))\n",
    "ax.imshow(real_data[0, :, :, 0])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Simulated real image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Fisher information matrix\n",
    "We calculate the Fisher information matrix by running the test data through the network. We need to shuffle and get a combination of the data before feeding it through the graph so that it has the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:34.972Z"
    }
   },
   "outputs": [],
   "source": [
    "ttd = n.shuffle(data = test_data[0], data_m = test_data[1], data_p = test_data[2], n_train = 1)\n",
    "tt, tt_m, tt_p = n.get_combination_data(data = ttd, combination = 0, n_batches = 1)\n",
    "F = n.sess.run(n.F, feed_dict = {n.x: tt, n.x_m: tt_m, n.x_p: tt_p, n.dropout: 1., n.dd: der_den})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC\n",
    "We now perform ABC by drawing 100000 random samples from the prior. We define the upper and lower bounds of a uniform prior to be 0 and 10. Only a uniform prior is implemented at the moment. From the samples we create simulations at each parameter value and feed each simulation through the network to get summaries. The summaries are compared to the summary of the real data to find the distances which can be used to accept or reject points.\n",
    "Because the simulations are created within the ABC function then the generation function must be passed. This is why the generator should be of the form defined above, which takes only a list of parameter values and returns a simulation at each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:35.656Z"
    }
   },
   "outputs": [],
   "source": [
    "θ, summary, s, ρ = n.ABC(real_data = real_data, F = F, prior = [0, 10], draws = 100000, generate_simulation = generate_data, at_once = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the simulations are going to be too large to make all at once the `at_once` option can be set to false which will create one simulation at a time.\n",
    "```python\n",
    "θ, summary, s, ρ = n.ABC(real_data = real_data, F = F, prior = [0, 10], draws = 100000, generate_simulation = generate_data, at_once = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept or reject\n",
    "In ABC draws are accepted if the distance between the simulation summary and the simulation of the real data are \"close\", i.e. smaller than some ϵ value, which is chosen somewhat arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:36.361Z"
    }
   },
   "outputs": [],
   "source": [
    "ϵ = 100\n",
    "accept_indices = np.argwhere(ρ < ϵ)[:, 0]\n",
    "reject_indices = np.argwhere(ρ >= ϵ)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples\n",
    "We can plot the output samples and the histogram of the accepted samples, which should peak around `θ = 1`. The monotonic function of all the output samples shows that the network has learned how to summarise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:37.124Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 10))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "ax[0].scatter(θ[accept_indices] , s[accept_indices, 0], s = 1)\n",
    "ax[0].scatter(θ[reject_indices], s[reject_indices, 0], s = 1, alpha = 0.1)\n",
    "ax[0].plot([0, 10], [summary[0], summary[0]], color = 'black', linestyle = 'dashed')\n",
    "ax[0].set_ylabel('Network output', labelpad = 0)\n",
    "ax[0].set_xlim([0, 10])\n",
    "ax[1].hist(θ[accept_indices], np.linspace(0, 10, 100), histtype = u'step', normed = True, linewidth = 1.5, color = '#9467bd');\n",
    "ax[1].set_xlabel('$\\\\theta$')\n",
    "ax[1].set_ylabel('$\\\\mathcal{P}(\\\\theta|{\\\\bf d})$')\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a lot of $\\theta$ draws which are unconstrained by the network because no similar structures were seen in the data which is indicative of using too small of a small training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMC-ABC\n",
    "Population Monte Carlo ABC is a way of reducing the number of draws by first sampling from a prior, accepting the closest 75% of the samples and weighting all the rest of the samples to create a new proposal distribution. The furthest 25% of the original samples are redrawn from the new proposal distribution. The furthest 25% of the simulation summaries are continually rejected and the proposal distribution updated until the number of draws needed accept all the 25% of the samples is much greater than this number of samples. This ratio is called the criterion. The inputs work in a very similar way to the `ABC` function above. If we want 1000 samples from the approximate distribution at the end of the PMC we need to set `num_keep = 1000`. The initial random draw (as in ABC above) initialises with `num_draws`, the larger this is the better proposal distribution will be on the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:38.271Z"
    }
   },
   "outputs": [],
   "source": [
    "θ_, summary_, ρ_, s_, W, total_draws = n.PMC(real_data = real_data, F = F, prior = [0, 10], num_draws = 1000, num_keep = 1000, generate_simulation = generate_data, criterion = 0.1, at_once = True, samples = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the PMC to continue for longer we can provide the output of PMC as an input as\n",
    "```python\n",
    "θ_, summary_, ρ_, s_, W, total_draws = n.PMC(real_data = real_data, F = F, prior = [0, 10], num_draws = 1000, num_keep = 1000, generate_simulation = generate_data, criterion = 0.001, at_once = True, samples = [θ_, summary_, ρ_, s_, W, total_draws])\n",
    "```\n",
    "Finally we can plot the accepted samples and plot their histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T13:49:39.134Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 10))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "ax[0].scatter(θ_ , s_, s = 1)\n",
    "ax[0].plot([0, 10], [summary[0], summary[0]], color = 'black', linestyle = 'dashed')\n",
    "ax[0].set_ylabel('Network output', labelpad = 0)\n",
    "ax[0].set_xlim([0, 10])\n",
    "ax[0].set_ylim([np.min(s_), np.max(s_)])\n",
    "ax[1].hist(θ_, np.linspace(0, 10, 100), histtype = u'step', normed = True, linewidth = 1.5, color = '#9467bd');\n",
    "ax[1].set_xlabel('θ')\n",
    "ax[1].set_ylabel('$\\\\mathcal{P}(\\\\theta|{\\\\bf d})$')\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't suffer from the unseen samples as much using the PMC because within a smaller $\\epsilon$ there are fewer of these unknown draws."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
