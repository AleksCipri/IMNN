{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information maximiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using neural networks, sufficient statistics can be obtained from data by maximising the Fisher information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this code please cite <a href=\"https://arxiv.org/abs/1802.03537\">arXiv:1802.03537</a>.<br><br>\n",
    "The code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:<br><br>\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1175196.svg)](https://doi.org/10.5281/zenodo.1175196)\n",
    "<br>\n",
    "The code presented below is version two (and is much more powerful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is run using<br>\n",
    ">`python-3.6.1`\n",
    "\n",
    ">`jupyter-1.0.0`\n",
    "\n",
    ">`tensorflow-1.8.0`\n",
    "\n",
    ">`numpy-1.14.2`\n",
    "\n",
    ">`tqdm==4.19.9`\n",
    "\n",
    ">`sys (native)`\n",
    "\n",
    "Although these precise versions may not be necessary, I have put them here to avoid possible conflicts. For reference, all code is run on a nVidia GeForce GTX 1080Ti (OC Strix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:29.576233Z",
     "start_time": "2018-08-02T14:05:21.534184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomcharnock/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IMNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "Here were going to use a 2D field of Gaussian noise with zero mean and unknown variance to see if the network can learn to summarise this variance.<br><br>\n",
    "We start by defining a function to generate the data with the correct shape. This should be \n",
    "```\n",
    "data_shape = None + input shape\n",
    "```\n",
    "It is useful to define this function so that it only takes in the value of the parameter as its input since the function can then be used for ABC later.<br><br>\n",
    "The data needs to be generated at a fiducial parameter value and at perturbed values just below and above the fiducial parameter for the numerical derivative. The data at the perturbed values should have the shape\n",
    "```\n",
    "perturbed_data_shape = None + number of parameters + input shape\n",
    "```\n",
    "\n",
    "We will choose the input shape to be a 10x20 pixel image with one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [10, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.675311Z",
     "start_time": "2018-08-02T14:05:40.628298Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(θ, train = False):\n",
    "    if train:\n",
    "        return np.moveaxis(np.random.normal(0., np.sqrt(θ), [1] + input_shape + [len(θ)]), -1, 0)\n",
    "    else:\n",
    "        return np.moveaxis(np.random.normal(0., np.sqrt(θ), input_shape + [len(θ)]), -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "Enough data needs to be made to approximate the covariance matrix of the output summaries. If the data is particularly large then several combinations need to be used (although this will lead to a less well approximated covariance matrix). We choose the number of simulations for each single combination to be 1000, and we will use all the data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.698588Z",
     "start_time": "2018-08-02T14:05:40.682691Z"
    }
   },
   "outputs": [],
   "source": [
    "n_s = 1000\n",
    "n_train = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fiducial parameter data can now be created, with variance θ = 1. We define how many simulations to use by passing the fiducial parameter through as a list. This is very useful for the ABC function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.773177Z",
     "start_time": "2018-08-02T14:05:40.705715Z"
    }
   },
   "outputs": [],
   "source": [
    "t = generate_data(θ = [1. for i in range(n_train * n_s)], train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A numerical derivative is used to calculate the derivative of the mean. This means we need to create simulations at values slightly above and below the central fiducial parameter value. By suppressing the sample variance between the simulations created at the lower and upper parameter values far fewer simulations are needed. We choose to use 5% of the total number of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_fraction = 0.05\n",
    "n_p = int(n_s * derivative_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample variance can be supressed by choosing the same initial seed when creating the upper and lower simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.846492Z",
     "start_time": "2018-08-02T14:05:40.778671Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = np.random.randint(1e6)\n",
    "np.random.seed(seed)\n",
    "t_m = generate_data(θ = [0.9 for i in range(n_train * n_p)], train = True)\n",
    "np.random.seed(seed)\n",
    "t_p = generate_data(θ = [1.1 for i in range(n_train * n_p)], train = True)\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the denominator of the derivative which is given by the difference between the perturbed parameter values<br><br>\n",
    "$$\\frac{\\partial}{\\partial\\theta} = \\frac{1}{1.1 - 0.9}.$$<br>\n",
    "This needs to be done for every parameter and kept in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.870562Z",
     "start_time": "2018-08-02T14:05:40.851383Z"
    }
   },
   "outputs": [],
   "source": [
    "derivative_denominator = 1. / 0.2\n",
    "der_den = np.array([derivative_denominator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass the data to the GPU in one go using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.883096Z",
     "start_time": "2018-08-02T14:05:40.875088Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\"x_central\": t, \"x_m\": t_m, \"x_p\":t_p}\n",
    "train_data = [t, t_m, t_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "We should also make some test data, although we don't need multiple combinations of this, i.e. `n_train = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:41.029099Z",
     "start_time": "2018-08-02T14:05:40.897450Z"
    }
   },
   "outputs": [],
   "source": [
    "tt = generate_data([1. for i in range(n_s)])\n",
    "seed = np.random.randint(1e6)\n",
    "np.random.seed(seed)\n",
    "tt_m = generate_data([0.9 for i in range(n_p)], train = True)\n",
    "np.random.seed(seed)\n",
    "tt_p = generate_data([1.1 for i in range(n_p)], train = True)\n",
    "np.random.seed()\n",
    "data[\"x_central_test\"] = tt\n",
    "data[\"x_m_test\"] = tt_m\n",
    "data[\"x_p_test\"] = tt_p\n",
    "test_data = [tt, tt_m, tt_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation\n",
    "We can simply plot a projection of some of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:43.602873Z",
     "start_time": "2018-08-02T14:05:42.122561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (20, 12))\n",
    "plt.subplots_adjust(wspace = 0)\n",
    "ax[0].imshow(data[\"x_central\"][np.random.randint(n_train * n_s), :, :, 0])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_xlabel('Training image')\n",
    "ax[1].imshow(data[\"x_central_test\"][np.random.randint(n_s), :, :, 0])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('Test image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliase the neural network\n",
    "### Define network parameters\n",
    "The network works with a base set of parameters which are<br>\n",
    "> `'verbose'` - `bool` - whether to print out diagnostics\n",
    "\n",
    "> `'number of simulations'` - `int` - the number of simulations to use in any one combination\n",
    "\n",
    "> `'number of combinations'` - `int` - the number of combinations to split simulations into\n",
    "\n",
    "> `'differentiation fraction'` - `float` - a fraction of the simulations to use for the numerical derivative\n",
    "\n",
    "> `'number of parameters'` - `int` - number of parameters in a model\n",
    "\n",
    "> `'number of summaries'` - `int` - number of summaries the network makes from the data\n",
    "\n",
    "> `'input shape'` - `int` or `list` - the number of inputs or the shape of the input if image-like input\n",
    "\n",
    "> `'preload data'` - `dict` or `None` - the training (and test) data to preload to the GPU in a ditionary, no preloading is done if `None`\n",
    "\n",
    "> `'prebuild'` - `bool` - whether to get the network to build a network or to provided your own\n",
    "\n",
    "> `'save file'` - `string` - a file name to save the graph (not saved if wrong type or not given)\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': n_s,\n",
    "    'number of combinations': n_train,\n",
    "    'differentiation fraction': derivative_fraction,\n",
    "    'number of parameters': 1,\n",
    "    'number of summaries': 1,\n",
    "    'input shape': input_shape,\n",
    "    'preload data': data,\n",
    "    'prebuild': False,\n",
    "}\n",
    "```\n",
    "The module can also build simple convolutional or dense networks (or a mixture of the two), which can be trigger by setting `'prebuild': True`. Several parameters are required to allow the network to build the network. These are<br>\n",
    "> `'wv'` - `float` - the variance with which to initialise the weights. If this is 0 or less, the network will determine the weight variance\n",
    "\n",
    "> `'bb'` - `float` - the constant value with which to initialise the biases\n",
    "\n",
    "> `'activation'` - `TensorFlow function` - a native tensorflow activation function\n",
    "\n",
    "> `'α'` - `float` or `int` - an additional parameter, if needed, for the tensorflow activation function\n",
    "\n",
    "> `'hidden layers'` - `list` - the architecture of the network. each element of the list is a hidden layer. A dense layer can be made using an integer where thet value indicates the number of neurons. A convolutional layer can be built by using a list where the first element is an integer where the number describes the number of filters, the second element is a list of the kernel size in the x and y directions, the third elemnet is a list of the strides in the x and y directions and the final element is string of 'SAME' or 'VALID' which describes the padding prescription.\n",
    "\n",
    "Here is an example of the IMNN which uses 1000 simulations per combination and 100 simulations per derivative for a model with one parameter where we require one summary. The module will build the network which takes in an input image of shape `[10, 20, 1]` and allows the network to decide the weight initialisation, initialises the biases at `bb = 0.1` and uses `tf.nn.leaky_relu` activation with a negative gradient parameter of `α = 0.01`. The network architecture is a convolution with 10 filters and a 5$\\times$5 kernel which does 2$\\times$2 strides, with 0-padding, followed by another convolution with 6 filters and 3$\\times$3 kernel with no striding and 0-padding. This is then followed by two dense layers with 100 neurons in each. We will save the graph into a file in the `data` folder called `saved_model.meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:29.652233Z",
     "start_time": "2018-08-02T14:05:29.620216Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': n_s,\n",
    "    'number of parameters': 1,\n",
    "    'differentiation fraction': derivative_fraction,\n",
    "    'number of summaries': 1,\n",
    "    'prebuild': True,\n",
    "    'input shape': input_shape,\n",
    "    'preload data': None,\n",
    "    'wv': 0.,\n",
    "    'bb': 0.1,\n",
    "    'activation': tf.nn.leaky_relu,\n",
    "    'α': 0.01,\n",
    "    'hidden layers': [[10, [5, 5], [2, 2], 'SAME'], [6, [3, 3], [1, 1], 'SAME'], 100, 100],\n",
    "}\n",
    "'''\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': n_s,\n",
    "    'number of combinations': n_train,\n",
    "    'number of parameters': 1,\n",
    "    'differentiation fraction': derivative_fraction,\n",
    "    'number of summaries': 1,\n",
    "    'prebuild': True,\n",
    "    'input shape': input_shape,\n",
    "    'preload data': data,\n",
    "    'wv': 0.,\n",
    "    'bb': 0.1,\n",
    "    'activation': tf.nn.leaky_relu,\n",
    "    'α': 0.01,\n",
    "    'hidden layers': [[10, [5, 5], [2, 2], 'SAME'], [6, [3, 3], [1, 1], 'SAME'], 100, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the network parameters are initialised using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:29.703601Z",
     "start_time": "2018-08-02T14:05:29.657272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model not being saved\n",
      "network architecture is [[10, 20, 1], [10, [5, 5], [2, 2], 'SAME'], [6, [3, 3], [1, 1], 'SAME'], 100, 100, 1].\n"
     ]
    }
   ],
   "source": [
    "n = IMNN.IMNN(parameters = parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "To build the network a learning rate, η, must be defined. The `setup(η)` function initialises the input tensors, builds the network and defines the optimisation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:05:40.605687Z",
     "start_time": "2018-08-02T14:05:32.443766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN/layer_1/conv_1/mul:0\", shape=(?, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN/layer_2/conv_2/mul:0\", shape=(?, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN/layer_3/dense_3/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN/layer_4/dense_4/mul:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"IMNN/layer_5/LeakyRelu:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"GatherNd:0\", shape=(1000, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1/conv_1/mul:0\", shape=(1000, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2/conv_2/mul:0\", shape=(1000, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3/dense_3/mul:0\", shape=(1000, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4/dense_4/mul:0\", shape=(1000, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5/LeakyRelu:0\", shape=(1000, 1), dtype=float32)\n",
      "Tensor(\"Const_3:0\", shape=(1000, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_1/LeakyRelu:0\", shape=(1000, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_1/LeakyRelu:0\", shape=(1000, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_1/LeakyRelu:0\", shape=(1000, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_1/LeakyRelu:0\", shape=(1000, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_1/LeakyRelu:0\", shape=(1000, 1), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(50, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_2/LeakyRelu:0\", shape=(50, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_2/LeakyRelu:0\", shape=(50, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_2/LeakyRelu:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_2/LeakyRelu:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_2/LeakyRelu:0\", shape=(50, 1), dtype=float32)\n",
      "Tensor(\"Reshape_3:0\", shape=(50, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_3/LeakyRelu:0\", shape=(50, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_3/LeakyRelu:0\", shape=(50, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_3/LeakyRelu:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_3/LeakyRelu:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_3/LeakyRelu:0\", shape=(50, 1), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(50, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_4/conv_1/mul:0\", shape=(50, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_4/conv_2/mul:0\", shape=(50, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_4/dense_3/mul:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_4/dense_4/mul:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_4/LeakyRelu:0\", shape=(50, 1), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(50, 10, 20, 1), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_1_5/conv_1/mul:0\", shape=(50, 5, 10, 10), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_2_5/conv_2/mul:0\", shape=(50, 5, 10, 6), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_3_5/dense_3/mul:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_4_5/dense_4/mul:0\", shape=(50, 100), dtype=float32)\n",
      "Tensor(\"IMNN_1/layer_5_5/LeakyRelu:0\", shape=(50, 1), dtype=float32)\n",
      "Tensor(\"central_output:0\", shape=(1, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_mean:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"central_difference_from_mean:0\", shape=(1, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_covariance:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"central_inverse_covariance:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"lower_output:0\", shape=(1, 50, 1, 1), dtype=float32)\n",
      "Tensor(\"upper_output:0\", shape=(1, 50, 1, 1), dtype=float32)\n",
      "Tensor(\"mean_derivative:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"fisher_information:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"central_output_1:0\", shape=(1, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_mean_1:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"central_difference_from_mean_1:0\", shape=(1, 1000, 1), dtype=float32)\n",
      "Tensor(\"central_covariance_1:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"central_inverse_covariance_1:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"lower_output_1:0\", shape=(1, 50, 1, 1), dtype=float32)\n",
      "Tensor(\"upper_output_1:0\", shape=(1, 50, 1, 1), dtype=float32)\n",
      "Tensor(\"mean_derivative_1:0\", shape=(1, 1, 1), dtype=float32)\n",
      "Tensor(\"fisher_information_1:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "n.setup(η = 1e-3, network = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-defined network\n",
    "A self defined network can be used instead of letting the module build the network for you. To do this you simply pass a function which contains the network to `setup(η = η, network = network)`. This function needs to take in two input tensors, the first is the shape of the input with `None` in the first axis and the second tensor is a tensorflow float (which will be the dropout). Since the weights need to be shared between several corresponding networks each set of trainable variables must be defined in its own scope. An example of the above network defined outside of the module is\n",
    "```python\n",
    "def network(input_tensor, dropout):\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        weights = tf.get_variable(\"weights\", [5, 5, 1, 10], initializer = tf.random_normal_initializer(0., 1.))\n",
    "        biases = tf.get_variable(\"biases\", [10], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.nn.conv2d(input_tensor, weights, [1, 2, 2, 1], padding = 'SAME')\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_2'):\n",
    "        weights = tf.get_variable(\"weights\", [3, 3, 10, 6], initializer = tf.random_normal_initializer(0., 1.))\n",
    "        biases = tf.get_variable(\"biases\", [6], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding = 'SAME')\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    x = tf.reshape(x, (-1, 300))\n",
    "    with tf.variable_scope('layer_3'):\n",
    "        weights = tf.get_variable(\"weights\", [300, 100], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 300)))\n",
    "        biases = tf.get_variable(\"biases\", [100], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_4'):\n",
    "        weights = tf.get_variable(\"weights\", [100, 100], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 100)))\n",
    "        biases = tf.get_variable(\"biases\", [100], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    x = tf.nn.dropout(x, dropout)\n",
    "    with tf.variable_scope('layer_5'):\n",
    "        weights = tf.get_variable(\"weights\", [100, 1], initializer = tf.random_normal_initializer(0., np.sqrt(2. / 100)))\n",
    "        biases = tf.get_variable(\"biases\", [1], initializer = tf.constant_initializer(0.1))\n",
    "    x = tf.matmul(x, weights)\n",
    "    x = tf.add(x, biases)\n",
    "    x = tf.nn.leaky_relu(x, 0.01)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing minimisation scheme\n",
    "By default the optimation scheme is<br>\n",
    "```python\n",
    "n.backpropagate = tf.train.GradientDescentOptimizer(η).minimize(Λ)\n",
    "```\n",
    "where Λ is the loss function tensor. To use any other training scheme, such as the `Adam` optimiser, it is sufficient to run\n",
    "```python\n",
    "n.backpropagate = tf.train.AdamOptimizer(η, β1, β2, ε).minimize(n.loss(n.F))\n",
    "```\n",
    "after `setup(η)` to override the default minimisation routine. If you want to continue to use the default minimisation routine but want to change the learning rate without reinitialising you can run \n",
    "```python\n",
    "n.training_scheme(η = new_η)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "With the data we can now easily train the network. The function simply takes the number of epochs, `num_epochs`, the fraction of neurons kept when using dropout `keep_rate`, and the denominator for the derivative calculated earlier, `der_den`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "keep_rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-02T14:05:42.989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240/500 [04:11<03:38,  1.19it/s, detF=0.237, detF_test=0.0234]"
     ]
    }
   ],
   "source": [
    "#train_F, test_F = n.train(train_data = train_data, num_epochs = num_epochs, n_train = n_train, num_batches = 1, keep_rate = keep_rate, der_den = der_den, test_data = test_data)\n",
    "train_F, test_F = n.train_with_preloaded(num_epochs, n_train, keep_rate, der_den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the the train function is a list of the determinant of the Fisher information at the end of each epoch of training on the train data, and the same on the test data if test data is provided.<br><br>\n",
    "These can be plotted, along with the loss function which is simply\n",
    "$$\\Lambda = -\\frac{1}{2}|F|^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:25.564805Z",
     "start_time": "2018-08-02T14:02:23.371429Z"
    }
   },
   "outputs": [],
   "source": [
    "train_F = np.array(train_F)\n",
    "test_F = np.array(test_F)\n",
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 14))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "end = len(train_F)\n",
    "epochs = np.arange(end)\n",
    "a, = ax[0].plot(epochs, -0.5 * train_F[:end]**2, label = 'Training data')\n",
    "b, = ax[0].plot(epochs, -test_F[:end]**2, label = 'Test data')\n",
    "ax[0].legend(frameon = False)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(epochs, train_F[:end])\n",
    "ax[1].plot(epochs, test_F[:end])\n",
    "ax[1].set_ylabel('$|\\mathcal{F}|$')\n",
    "ax[1].set_xlabel('Number of epochs')\n",
    "ax[1].set_xlim([0, len(epochs)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test loss deviates from the training loss. This is to be expected because there are will be a lot of correlation within a small training set which isn't in the test set. As long as the test loss doesn't start increasing then it is likely that the network is still working, with the maximum Fisher available being the value obtained from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the network\n",
    "If you need to reset the weights and biases for any reason then you can call\n",
    "```python\n",
    "n.reinitialise_session()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the network\n",
    "\n",
    "If you don't initialise the network with a save name you can save the network as a `TensorFlow` `meta` graph. For example saving the model in the directory `/.data` called `saved_model.meta` can be done using the function\n",
    "```python\n",
    "n.save_network(file_name = \"data/saved_model\", first_time = True)\n",
    "```\n",
    "If `save file` is passed with a correct file name when initialising the module then the initialised network will be saved by\n",
    "```python\n",
    "n.begin_session()\n",
    "```\n",
    "and then saved at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the network\n",
    "\n",
    "You can load the network from a `TensorFlow` `meta` graph (from `/.data/saved_model.meta`) using\n",
    "```python\n",
    "parameters = {\n",
    "    'verbose': True,\n",
    "    'number of simulations': 500,\n",
    "    'differentiation fraction': .1,\n",
    "    'number of parameters': 1,\n",
    "    'number of summaries': 1,\n",
    "    'input shape': [10, 20, 1],\n",
    "    'prebuild': False,\n",
    "    'save file': \"data/saved_model\",\n",
    "}\n",
    "```\n",
    "and then running\n",
    "```python\n",
    "n = IMNN.IMNN(parameters = parameters)\n",
    "n.restore_network()\n",
    "```\n",
    "Training can be continued after restoring the model - although the Adam optimiser might need to reacquaint itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Bayesian computation\n",
    "We can now do ABC (or PMC-ABC) with our calculated summary. First we generate some simulated real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:25.588242Z",
     "start_time": "2018-08-02T14:02:25.575238Z"
    }
   },
   "outputs": [],
   "source": [
    "real_data = generate_data(θ = [1.], train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this real data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:26.089352Z",
     "start_time": "2018-08-02T14:02:25.594414Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 12))\n",
    "ax.imshow(real_data[0, :, :, 0])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Simulated real image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Fisher information matrix\n",
    "We calculate the Fisher information matrix by running the test data through the network. We need to shuffle and get a combination of the data before feeding it through the graph so that it has the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:26.255087Z",
     "start_time": "2018-08-02T14:02:26.098316Z"
    }
   },
   "outputs": [],
   "source": [
    "#ttd = n.shuffle(data = test_data[0], data_m = test_data[1], data_p = test_data[2], n_train = 1)\n",
    "#tt, tt_m, tt_p = n.get_combination_data(data = ttd, combination = 0, n_batches = 1)\n",
    "#F = n.sess.run(n.F, feed_dict = {n.x: tt, n.x_m: tt_m, n.x_p: tt_p, n.dropout: 1., n.dd: der_den})\n",
    "F = n.sess.run(n.test_F, feed_dict = {n.dd: der_den})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC\n",
    "We now perform ABC by drawing 100000 random samples from the prior. We define the upper and lower bounds of a uniform prior to be 0 and 10. Only a uniform prior is implemented at the moment. From the samples we create simulations at each parameter value and feed each simulation through the network to get summaries. The summaries are compared to the summary of the real data to find the distances which can be used to accept or reject points.\n",
    "Because the simulations are created within the ABC function then the generation function must be passed. This is why the generator should be of the form defined above, which takes only a list of parameter values and returns a simulation at each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:44.590019Z",
     "start_time": "2018-08-02T14:02:26.270126Z"
    }
   },
   "outputs": [],
   "source": [
    "θ, summary, s, ρ = n.ABC(real_data = real_data, F = F, prior = [0, 10], draws = 100000, generate_simulation = generate_data, at_once = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the simulations are going to be too large to make all at once the `at_once` option can be set to false which will create one simulation at a time.\n",
    "```python\n",
    "θ, summary, s, ρ = n.ABC(real_data = real_data, F = F, prior = [0, 10], draws = 100000, generate_simulation = generate_data, at_once = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept or reject\n",
    "In ABC draws are accepted if the distance between the simulation summary and the simulation of the real data are \"close\", i.e. smaller than some ϵ value, which is chosen somewhat arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:44.612830Z",
     "start_time": "2018-08-02T14:02:44.596659Z"
    }
   },
   "outputs": [],
   "source": [
    "ϵ = 5\n",
    "accept_indices = np.argwhere(ρ < ϵ)[:, 0]\n",
    "reject_indices = np.argwhere(ρ >= ϵ)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples\n",
    "We can plot the output samples and the histogram of the accepted samples, which should peak around `θ = 1`. The monotonic function of all the output samples shows that the network has learned how to summarise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:02:50.854609Z",
     "start_time": "2018-08-02T14:02:44.618677Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 10))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "ax[0].scatter(θ[accept_indices] , s[accept_indices, 0], s = 1)\n",
    "ax[0].scatter(θ[reject_indices], s[reject_indices, 0], s = 1, alpha = 0.1)\n",
    "ax[0].plot([0, 10], [summary[0], summary[0]], color = 'black', linestyle = 'dashed')\n",
    "ax[0].set_ylabel('Network output', labelpad = 0)\n",
    "ax[0].set_xlim([0, 10])\n",
    "ax[1].hist(θ[accept_indices], np.linspace(0, 10, 100), histtype = u'step', normed = True, linewidth = 1.5, color = '#9467bd');\n",
    "ax[1].set_xlabel('$\\\\theta$')\n",
    "ax[1].set_ylabel('$\\\\mathcal{P}(\\\\theta|{\\\\bf d})$')\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a lot of $\\theta$ draws which are unconstrained by the network because no similar structures were seen in the data which is indicative of using too small of a small training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMC-ABC\n",
    "Population Monte Carlo ABC is a way of reducing the number of draws by first sampling from a prior, accepting the closest 75% of the samples and weighting all the rest of the samples to create a new proposal distribution. The furthest 25% of the original samples are redrawn from the new proposal distribution. The furthest 25% of the simulation summaries are continually rejected and the proposal distribution updated until the number of draws needed accept all the 25% of the samples is much greater than this number of samples. This ratio is called the criterion. The inputs work in a very similar way to the `ABC` function above. If we want 1000 samples from the approximate distribution at the end of the PMC we need to set `num_keep = 1000`. The initial random draw (as in ABC above) initialises with `num_draws`, the larger this is the better proposal distribution will be on the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:03:27.657263Z",
     "start_time": "2018-08-02T14:02:50.922469Z"
    }
   },
   "outputs": [],
   "source": [
    "θ_, summary_, ρ_, s_, W, total_draws = n.PMC(real_data = real_data, F = F, prior = [0, 10], num_draws = 1000, num_keep = 1000, generate_simulation = generate_data, criterion = 0.05, at_once = True, samples = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the PMC to continue for longer we can provide the output of PMC as an input as\n",
    "```python\n",
    "θ_, summary_, ρ_, s_, W, total_draws = n.PMC(real_data = real_data, F = F, prior = [0, 10], num_draws = 1000, num_keep = 1000, generate_simulation = generate_data, criterion = 0.001, at_once = True, samples = [θ_, summary_, ρ_, s_, W, total_draws])\n",
    "```\n",
    "Finally we can plot the accepted samples and plot their histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T14:03:28.599156Z",
     "start_time": "2018-08-02T14:03:27.679407Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex = True, figsize = (10, 10))\n",
    "plt.subplots_adjust(hspace = 0)\n",
    "ax[0].scatter(θ_ , s_, s = 1)\n",
    "ax[0].plot([0, 10], [summary[0], summary[0]], color = 'black', linestyle = 'dashed')\n",
    "ax[0].set_ylabel('Network output', labelpad = 0)\n",
    "ax[0].set_xlim([0, 10])\n",
    "ax[0].set_ylim([np.min(s_), np.max(s_)])\n",
    "ax[1].hist(θ_, np.linspace(0, 10, 100), histtype = u'step', normed = True, linewidth = 1.5, color = '#9467bd');\n",
    "ax[1].set_xlabel('θ')\n",
    "ax[1].set_ylabel('$\\\\mathcal{P}(\\\\theta|{\\\\bf d})$')\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't suffer from the unseen samples as much using the PMC because within a smaller $\\epsilon$ there are fewer of these unknown draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
