{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s = 1000\n",
    "n_d = 100\n",
    "n_train = 10\n",
    "n_params = 1\n",
    "n_summaries = 2\n",
    "input_shape = (10,)\n",
    "t_fid = np.array([1.])\n",
    "dt = np.array([0.1])\n",
    "ns = tf.constant(n_s, dtype=tf.float32)\n",
    "nd = tf.constant(n_d, dtype=tf.float32)\n",
    "θ_fid = tf.constant(t_fid, dtype=tf.float32)\n",
    "dθ = tf.constant(dt, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "simulations = np.random.normal(0., np.sqrt(t_fid[0]), (n_train * max(n_s, n_d),) + input_shape)[:n_train * n_s].astype(np.float32)\n",
    "np.random.seed(seed)\n",
    "simulations_m = np.random.normal(0., np.sqrt(t_fid[0] - dt[0]), (n_train * max(n_s, n_d), n_params) + input_shape)[:n_train * n_d].astype(np.float32)\n",
    "np.random.seed(seed)\n",
    "simulations_p = np.random.normal(0., np.sqrt(t_fid[0] + dt[0]), (n_train * max(n_s, n_d), n_params) + input_shape)[:n_train * n_d].astype(np.float32)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "test_simulations = np.random.normal(0., np.sqrt(t_fid[0]), (max(n_s, n_d),) + input_shape)[:n_s].astype(np.float32)\n",
    "np.random.seed(seed)\n",
    "test_simulations_m = np.random.normal(0., np.sqrt(t_fid[0] - dt[0]), (max(n_s, n_d), n_params) + input_shape)[:n_d].astype(np.float32)\n",
    "np.random.seed(seed)\n",
    "test_simulations_p = np.random.normal(0., np.sqrt(t_fid[0] + dt[0]), (max(n_s, n_d), n_params) + input_shape)[:n_d].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_at_once = 17\n",
    "batches = np.ceil(n_s / sims_at_once).astype(int)\n",
    "\n",
    "train_sims = tf.data.Dataset.from_tensor_slices(simulations)\n",
    "train_sims = train_sims.batch(n_s).shuffle(n_s * n_train, reshuffle_each_iteration=False)\n",
    "train_sims = train_sims.map(lambda x: tf.data.Dataset.from_tensor_slices(x).repeat(2))\n",
    "#train_sims = train_sims.map(lambda x: tf.data.Dataset.from_tensor_slices(x).batch(sims_at_once))\n",
    "\n",
    "\n",
    "\n",
    "#sims_inds = tf.data.Dataset.from_tensor_slices(np.arange(n_s)[:, np.newaxis].astype(np.int32))\n",
    "#sims_inds = sims_inds.batch(sims_at_once)\n",
    "\n",
    "#ders_at_once = n_d\n",
    "\n",
    "#train_ders = tf.data.Dataset.from_tensor_slices((simulations_m, simulations_p))\n",
    "#train_ders = train_ders.batch(ders_at_once).shuffle(10 * n_d)\n",
    "#der_inds = tf.data.Dataset.from_tensor_slices(np.arange(n_d)[:, np.newaxis].astype(np.int32))\n",
    "#der_inds = der_inds.batch(ders_at_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_iterator = iter(train_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iterator = iter(next(sims_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 10)\n"
     ]
    }
   ],
   "source": [
    "print(next(batch_iterator).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staggered_train(train_sims, sims_inds, train_ders, der_inds, λ, α):\n",
    "    x, x_m, x_p = get_summaries(train_sims, sims_inds, train_ders, der_inds)\n",
    "    C, μ_offset = get_covariance(x)\n",
    "    Cinv = tf.linalg.inv(C)\n",
    "    dμdθ = get_derivative_mean(x_m, x_p)\n",
    "    F = get_Fisher(Cinv, dμdθ)\n",
    "    \n",
    "    dCdx = get_covariance_derivative(μ_offset)\n",
    "    dΛdx, dΛdx_m, dΛdx_p = get_log_det_fisher_derivative(F, Cinv, dμdθ, dCdx)\n",
    "    dΛ_2dx, r = get_regularisation_derivative(C, Cinv, dCdx, λ, α)\n",
    "    dΛdx = tf.add(dΛ_2dx, dΛdx)\n",
    "    \n",
    "    for i in sims_inds:\n",
    "        print(i)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_sims:\n",
    "    counter = 0\n",
    "    a = []\n",
    "    b = []\n",
    "    for j in i:\n",
    "        a.append(j)\n",
    "        counter += 1\n",
    "        if counter >= batches:\n",
    "            break\n",
    "    for j in i:\n",
    "        b.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = tf.data.Dataset.from_tensor_slices(test_simulations)\n",
    "test_sims = test_sims.batch(sims_at_once)\n",
    "test_ders = tf.data.Dataset.from_tensor_slices((test_simulations_m, test_simulations_p))\n",
    "test_ders = test_ders.batch(ders_at_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [tf.keras.Input(shape=(10,)),\n",
    "     tf.keras.layers.Dense(128),\n",
    "     tf.keras.layers.Activation(\"tanh\"),\n",
    "     tf.keras.layers.Dense(128),\n",
    "     tf.keras.layers.Activation(\"tanh\"),\n",
    "     tf.keras.layers.Dense(n_summaries),\n",
    "     tf.keras.layers.Activation(\"tanh\")\n",
    "    ])\n",
    "opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_summaries(train_sims, sims_inds, train_ders, der_inds):\n",
    "    x = tf.zeros(shape=(n_s, n_summaries))\n",
    "    for i in sims_inds:\n",
    "        s = next(iter(train_sims))\n",
    "        x = tf.tensor_scatter_nd_update(x, i, model(s[: tf.shape(i)[0]]))\n",
    "    x_m = tf.zeros(shape=(n_d, n_params, n_summaries))\n",
    "    x_p = tf.zeros(shape=(n_d, n_params, n_summaries))\n",
    "    for i in der_inds:\n",
    "        s_m, s_p = next(iter(train_ders))\n",
    "        x_m = tf.tensor_scatter_nd_update(x_m, i, model(s_m[: tf.shape(i)[0]]))\n",
    "        x_p = tf.tensor_scatter_nd_update(x_p, i, model(s_p[: tf.shape(i)[0]]))\n",
    "    return x, x_m, x_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance(x):\n",
    "    μ = tf.reduce_mean(x, \n",
    "                       axis=0, \n",
    "                       keepdims=True)\n",
    "    μ_offset = tf.subtract(x, μ)\n",
    "    \n",
    "    C = tf.divide(\n",
    "        tf.einsum(\n",
    "            \"ij,ik->jk\", \n",
    "            μ_offset, \n",
    "            μ_offset), \n",
    "        tf.subtract(ns, 1.))\n",
    "    return C, μ_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_mean(x_m, x_p):\n",
    "    return tf.reduce_mean(\n",
    "        tf.einsum(\n",
    "            \"ijk,j->ijk\",\n",
    "            tf.subtract(x_p, x_m),\n",
    "            tf.divide(\n",
    "                1.,\n",
    "                tf.multiply(2., dθ))),\n",
    "            axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Fisher(Cinv, dμdθ):\n",
    "    F = tf.linalg.band_part(\n",
    "            tf.einsum(\n",
    "                \"ij,kj->ik\", \n",
    "                dμdθ, \n",
    "                tf.einsum(\n",
    "                    \"ij,kj->ki\", \n",
    "                    Cinv, \n",
    "                    dμdθ)), 0, -1)\n",
    "    return tf.multiply(\n",
    "        0.5, \n",
    "        tf.add(\n",
    "            F, \n",
    "            tf.transpose(F, perm=[1, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_derivative(μ_offset):\n",
    "    dxdx = tf.einsum(\n",
    "        \"ij,kl->ijkl\", \n",
    "        tf.eye(n_s, n_s), \n",
    "        tf.eye(n_summaries, n_summaries))\n",
    "    dμ_offsetdx = tf.subtract(\n",
    "        dxdx, \n",
    "        tf.reduce_mean(\n",
    "            dxdx, \n",
    "            axis=0, \n",
    "            keepdims=True))\n",
    "    return tf.divide(\n",
    "        tf.reduce_sum(\n",
    "            tf.add(\n",
    "                tf.einsum(\n",
    "                    \"ijkl,im->ijkml\", \n",
    "                    dμ_offsetdx, \n",
    "                    μ_offset),\n",
    "                tf.einsum(\n",
    "                    \"ij,iklm->ikjlm\",\n",
    "                    μ_offset, \n",
    "                    dμ_offsetdx)),\n",
    "            axis=0),\n",
    "        tf.subtract(ns, 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_mean_derivative():\n",
    "    dxdx = tf.einsum(\n",
    "        \"ij,kl,mn->ijklmn\", \n",
    "        tf.eye(n_d, n_d),\n",
    "        tf.eye(n_params, n_params),\n",
    "        tf.eye(n_summaries, n_summaries))\n",
    "    ddμdθdx = tf.reduce_mean(\n",
    "        tf.einsum(\n",
    "            \"ijklmn,l->ijkmnl\",\n",
    "            dxdx, \n",
    "            tf.divide(\n",
    "                1., \n",
    "                tf.multiply(2., dθ))), \n",
    "        axis=0)\n",
    "    return ddμdθdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fisher_derivative(Cinv, dμdθ, dCdx):\n",
    "    ddμdθdx = get_derivative_mean_derivative()\n",
    "    Cinvdμdθ = tf.einsum(\n",
    "        \"ij,kj->ki\", \n",
    "        Cinv, \n",
    "        dμdθ)\n",
    "    dFdx = tf.linalg.band_part(\n",
    "        tf.einsum(\n",
    "            \"ij,kljm->kmil\",\n",
    "            dμdθ, \n",
    "            tf.einsum(\n",
    "                \"ij,kljm->klim\",\n",
    "                Cinv, \n",
    "                tf.einsum(\n",
    "                    \"ijkl,mk->imjl\",\n",
    "                    dCdx, \n",
    "                    Cinvdμdθ))), 0, -1)\n",
    "    dFdx = tf.multiply(\n",
    "        -0.5,\n",
    "        tf.add(\n",
    "            dFdx, \n",
    "            tf.transpose(\n",
    "                dFdx, \n",
    "                perm=[0, 1, 3, 2])))\n",
    "    \n",
    "    dFdx_m_a = tf.linalg.band_part(\n",
    "            tf.einsum(\n",
    "                \"ijklm,nk->imljn\",\n",
    "                -ddμdθdx, \n",
    "                Cinvdμdθ), 0, -1)\n",
    "    dFdx_m_b = tf.linalg.band_part(\n",
    "            tf.einsum(\n",
    "                \"ij,kljmn->knmil\",\n",
    "                dμdθ, \n",
    "                tf.einsum(\n",
    "                    \"ij,kljmn->klimn\",\n",
    "                    Cinv, \n",
    "                    -ddμdθdx)), 0, -1)  \n",
    "    dFdx_m = tf.multiply(\n",
    "        0.5,\n",
    "        tf.add(\n",
    "            tf.add(\n",
    "                dFdx_m_a,\n",
    "                tf.transpose(\n",
    "                    dFdx_m_a,\n",
    "                    perm=[0, 1, 2, 4, 3])),\n",
    "            tf.add(\n",
    "                dFdx_m_b,\n",
    "                tf.transpose(\n",
    "                    dFdx_m_b,\n",
    "                    perm=[0, 1, 2, 4, 3]))))\n",
    "    \n",
    "    dFdx_p_a = tf.linalg.band_part(\n",
    "            tf.einsum(\n",
    "                \"ijklm,nk->imljn\",\n",
    "                ddμdθdx, \n",
    "                Cinvdμdθ), 0, -1)\n",
    "    dFdx_p_b = tf.linalg.band_part(\n",
    "            tf.einsum(\n",
    "                \"ij,kljmn->knmil\",\n",
    "                dμdθ, \n",
    "                tf.einsum(\n",
    "                    \"ij,kljmn->klimn\",\n",
    "                    Cinv, \n",
    "                    ddμdθdx)), 0, -1)  \n",
    "    dFdx_p = tf.multiply(\n",
    "        0.5,\n",
    "        tf.add(\n",
    "            tf.add(\n",
    "                dFdx_p_a,\n",
    "                tf.transpose(\n",
    "                    dFdx_p_a,\n",
    "                    perm=[0, 1, 2, 4, 3])),\n",
    "            tf.add(\n",
    "                dFdx_p_b,\n",
    "                tf.transpose(\n",
    "                    dFdx_p_b,\n",
    "                    perm=[0, 1, 2, 4, 3]))))\n",
    "    return dFdx, dFdx_m, dFdx_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_det_fisher_derivative(F, Cinv, dμdθ, dCdx):\n",
    "    Finv = tf.linalg.inv(F)\n",
    "    dFdx, dFdx_m, dFdx_p = get_fisher_derivative(Cinv, dμdθ, dCdx)\n",
    "    \n",
    "    dΛdx = -tf.linalg.trace(\n",
    "        tf.einsum(\n",
    "            \"ij,kljm->klim\",\n",
    "            Finv, \n",
    "            dFdx))\n",
    "    dΛdx_m = -tf.linalg.trace(\n",
    "        tf.einsum(\n",
    "            \"ij,klmjn->klmin\",\n",
    "            Finv, \n",
    "            dFdx_m))\n",
    "    dΛdx_p = -tf.linalg.trace(\n",
    "        tf.einsum(\n",
    "            \"ij,klmjn->klmin\", \n",
    "            Finv, \n",
    "            dFdx_p))\n",
    "    return dΛdx, dΛdx_m, dΛdx_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularisation(C, Cinv):\n",
    "    I = tf.eye(n_summaries)\n",
    "    CmI = tf.subtract(C, I)\n",
    "    CinvmI = tf.subtract(Cinv, I)\n",
    "    Λ_2 = tf.multiply(0.5,\n",
    "        tf.add(\n",
    "            tf.square(\n",
    "                tf.norm(CmI, \n",
    "                        ord=\"fro\", \n",
    "                        axis=(0, 1))),\n",
    "            tf.square(\n",
    "                tf.norm(CinvmI, \n",
    "                        ord=\"fro\", \n",
    "                        axis=(0, 1)))))\n",
    "    return Λ_2, CmI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularisation_strength(Λ_2, λ, α):\n",
    "    rate = tf.multiply(-α, Λ_2)\n",
    "    e_rate = tf.exp(rate)\n",
    "    r = tf.divide(\n",
    "            tf.multiply(\n",
    "                λ, \n",
    "                Λ_2), \n",
    "            tf.add(\n",
    "                Λ_2, \n",
    "                e_rate))\n",
    "    drdΛ_2 = tf.multiply(\n",
    "        r, \n",
    "        tf.add(\n",
    "            1., \n",
    "            tf.divide(\n",
    "                tf.multiply(\n",
    "                    tf.add(\n",
    "                        1.,\n",
    "                        tf.multiply(\n",
    "                            α,\n",
    "                            Λ_2)),\n",
    "                    e_rate),\n",
    "                tf.add(\n",
    "                    Λ_2,\n",
    "                    e_rate))))\n",
    "    return r, drdΛ_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularisation_derivative(C, Cinv, dCdx, λ, α):\n",
    "    Λ_2, CmI = get_regularisation(C, Cinv)\n",
    "    r, drdΛ_2= get_regularisation_strength(Λ_2, λ, α)\n",
    "    Cinv2 = tf.einsum(\n",
    "        \"ij,jk->ik\", \n",
    "        Cinv, \n",
    "        Cinv)\n",
    "    Cinv3 = tf.einsum(\n",
    "        \"ij,jk->ik\",\n",
    "        Cinv2,\n",
    "        Cinv)\n",
    "    dΛ_2dx = tf.multiply(\n",
    "        tf.add(\n",
    "            tf.multiply(\n",
    "                Λ_2,\n",
    "                drdΛ_2),\n",
    "            r),            \n",
    "        tf.linalg.trace(\n",
    "            tf.einsum(\n",
    "                \"ij,kjlm->kilm\",\n",
    "                tf.add(\n",
    "                    CmI, \n",
    "                    tf.subtract(\n",
    "                        Cinv2, \n",
    "                        Cinv3)),\n",
    "                dCdx)))\n",
    "    return dΛ_2dx, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  0]\n",
      " [  1]\n",
      " [  2]\n",
      " [  3]\n",
      " [  4]\n",
      " [  5]\n",
      " [  6]\n",
      " [  7]\n",
      " [  8]\n",
      " [  9]\n",
      " [ 10]\n",
      " [ 11]\n",
      " [ 12]\n",
      " [ 13]\n",
      " [ 14]\n",
      " [ 15]\n",
      " [ 16]\n",
      " [ 17]\n",
      " [ 18]\n",
      " [ 19]\n",
      " [ 20]\n",
      " [ 21]\n",
      " [ 22]\n",
      " [ 23]\n",
      " [ 24]\n",
      " [ 25]\n",
      " [ 26]\n",
      " [ 27]\n",
      " [ 28]\n",
      " [ 29]\n",
      " [ 30]\n",
      " [ 31]\n",
      " [ 32]\n",
      " [ 33]\n",
      " [ 34]\n",
      " [ 35]\n",
      " [ 36]\n",
      " [ 37]\n",
      " [ 38]\n",
      " [ 39]\n",
      " [ 40]\n",
      " [ 41]\n",
      " [ 42]\n",
      " [ 43]\n",
      " [ 44]\n",
      " [ 45]\n",
      " [ 46]\n",
      " [ 47]\n",
      " [ 48]\n",
      " [ 49]\n",
      " [ 50]\n",
      " [ 51]\n",
      " [ 52]\n",
      " [ 53]\n",
      " [ 54]\n",
      " [ 55]\n",
      " [ 56]\n",
      " [ 57]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 60]\n",
      " [ 61]\n",
      " [ 62]\n",
      " [ 63]\n",
      " [ 64]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 72]\n",
      " [ 73]\n",
      " [ 74]\n",
      " [ 75]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 84]\n",
      " [ 85]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 88]\n",
      " [ 89]\n",
      " [ 90]\n",
      " [ 91]\n",
      " [ 92]\n",
      " [ 93]\n",
      " [ 94]\n",
      " [ 95]\n",
      " [ 96]\n",
      " [ 97]\n",
      " [ 98]\n",
      " [ 99]\n",
      " [100]\n",
      " [101]\n",
      " [102]\n",
      " [103]\n",
      " [104]\n",
      " [105]\n",
      " [106]\n",
      " [107]\n",
      " [108]\n",
      " [109]\n",
      " [110]\n",
      " [111]\n",
      " [112]\n",
      " [113]\n",
      " [114]\n",
      " [115]\n",
      " [116]\n",
      " [117]\n",
      " [118]\n",
      " [119]\n",
      " [120]\n",
      " [121]\n",
      " [122]\n",
      " [123]\n",
      " [124]\n",
      " [125]\n",
      " [126]\n",
      " [127]\n",
      " [128]\n",
      " [129]\n",
      " [130]\n",
      " [131]\n",
      " [132]\n",
      " [133]\n",
      " [134]\n",
      " [135]\n",
      " [136]\n",
      " [137]\n",
      " [138]\n",
      " [139]\n",
      " [140]\n",
      " [141]\n",
      " [142]\n",
      " [143]\n",
      " [144]\n",
      " [145]\n",
      " [146]\n",
      " [147]\n",
      " [148]\n",
      " [149]\n",
      " [150]\n",
      " [151]\n",
      " [152]\n",
      " [153]\n",
      " [154]\n",
      " [155]\n",
      " [156]\n",
      " [157]\n",
      " [158]\n",
      " [159]\n",
      " [160]\n",
      " [161]\n",
      " [162]\n",
      " [163]\n",
      " [164]\n",
      " [165]\n",
      " [166]\n",
      " [167]\n",
      " [168]\n",
      " [169]\n",
      " [170]\n",
      " [171]\n",
      " [172]\n",
      " [173]\n",
      " [174]\n",
      " [175]\n",
      " [176]\n",
      " [177]\n",
      " [178]\n",
      " [179]\n",
      " [180]\n",
      " [181]\n",
      " [182]\n",
      " [183]\n",
      " [184]\n",
      " [185]\n",
      " [186]\n",
      " [187]\n",
      " [188]\n",
      " [189]\n",
      " [190]\n",
      " [191]\n",
      " [192]\n",
      " [193]\n",
      " [194]\n",
      " [195]\n",
      " [196]\n",
      " [197]\n",
      " [198]\n",
      " [199]\n",
      " [200]\n",
      " [201]\n",
      " [202]\n",
      " [203]\n",
      " [204]\n",
      " [205]\n",
      " [206]\n",
      " [207]\n",
      " [208]\n",
      " [209]\n",
      " [210]\n",
      " [211]\n",
      " [212]\n",
      " [213]\n",
      " [214]\n",
      " [215]\n",
      " [216]\n",
      " [217]\n",
      " [218]\n",
      " [219]\n",
      " [220]\n",
      " [221]\n",
      " [222]\n",
      " [223]\n",
      " [224]\n",
      " [225]\n",
      " [226]\n",
      " [227]\n",
      " [228]\n",
      " [229]\n",
      " [230]\n",
      " [231]\n",
      " [232]\n",
      " [233]\n",
      " [234]\n",
      " [235]\n",
      " [236]\n",
      " [237]\n",
      " [238]\n",
      " [239]\n",
      " [240]\n",
      " [241]\n",
      " [242]\n",
      " [243]\n",
      " [244]\n",
      " [245]\n",
      " [246]\n",
      " [247]\n",
      " [248]\n",
      " [249]\n",
      " [250]\n",
      " [251]\n",
      " [252]\n",
      " [253]\n",
      " [254]\n",
      " [255]\n",
      " [256]\n",
      " [257]\n",
      " [258]\n",
      " [259]\n",
      " [260]\n",
      " [261]\n",
      " [262]\n",
      " [263]\n",
      " [264]\n",
      " [265]\n",
      " [266]\n",
      " [267]\n",
      " [268]\n",
      " [269]\n",
      " [270]\n",
      " [271]\n",
      " [272]\n",
      " [273]\n",
      " [274]\n",
      " [275]\n",
      " [276]\n",
      " [277]\n",
      " [278]\n",
      " [279]\n",
      " [280]\n",
      " [281]\n",
      " [282]\n",
      " [283]\n",
      " [284]\n",
      " [285]\n",
      " [286]\n",
      " [287]\n",
      " [288]\n",
      " [289]\n",
      " [290]\n",
      " [291]\n",
      " [292]\n",
      " [293]\n",
      " [294]\n",
      " [295]\n",
      " [296]\n",
      " [297]\n",
      " [298]\n",
      " [299]\n",
      " [300]\n",
      " [301]\n",
      " [302]\n",
      " [303]\n",
      " [304]\n",
      " [305]\n",
      " [306]\n",
      " [307]\n",
      " [308]\n",
      " [309]\n",
      " [310]\n",
      " [311]\n",
      " [312]\n",
      " [313]\n",
      " [314]\n",
      " [315]\n",
      " [316]\n",
      " [317]\n",
      " [318]\n",
      " [319]\n",
      " [320]\n",
      " [321]\n",
      " [322]\n",
      " [323]\n",
      " [324]\n",
      " [325]\n",
      " [326]\n",
      " [327]\n",
      " [328]\n",
      " [329]\n",
      " [330]\n",
      " [331]\n",
      " [332]\n",
      " [333]\n",
      " [334]\n",
      " [335]\n",
      " [336]\n",
      " [337]\n",
      " [338]\n",
      " [339]\n",
      " [340]\n",
      " [341]\n",
      " [342]\n",
      " [343]\n",
      " [344]\n",
      " [345]\n",
      " [346]\n",
      " [347]\n",
      " [348]\n",
      " [349]\n",
      " [350]\n",
      " [351]\n",
      " [352]\n",
      " [353]\n",
      " [354]\n",
      " [355]\n",
      " [356]\n",
      " [357]\n",
      " [358]\n",
      " [359]\n",
      " [360]\n",
      " [361]\n",
      " [362]\n",
      " [363]\n",
      " [364]\n",
      " [365]\n",
      " [366]\n",
      " [367]\n",
      " [368]\n",
      " [369]\n",
      " [370]\n",
      " [371]\n",
      " [372]\n",
      " [373]\n",
      " [374]\n",
      " [375]\n",
      " [376]\n",
      " [377]\n",
      " [378]\n",
      " [379]\n",
      " [380]\n",
      " [381]\n",
      " [382]\n",
      " [383]\n",
      " [384]\n",
      " [385]\n",
      " [386]\n",
      " [387]\n",
      " [388]\n",
      " [389]\n",
      " [390]\n",
      " [391]\n",
      " [392]\n",
      " [393]\n",
      " [394]\n",
      " [395]\n",
      " [396]\n",
      " [397]\n",
      " [398]\n",
      " [399]\n",
      " [400]\n",
      " [401]\n",
      " [402]\n",
      " [403]\n",
      " [404]\n",
      " [405]\n",
      " [406]\n",
      " [407]\n",
      " [408]\n",
      " [409]\n",
      " [410]\n",
      " [411]\n",
      " [412]\n",
      " [413]\n",
      " [414]\n",
      " [415]\n",
      " [416]\n",
      " [417]\n",
      " [418]\n",
      " [419]\n",
      " [420]\n",
      " [421]\n",
      " [422]\n",
      " [423]\n",
      " [424]\n",
      " [425]\n",
      " [426]\n",
      " [427]\n",
      " [428]\n",
      " [429]\n",
      " [430]\n",
      " [431]\n",
      " [432]\n",
      " [433]\n",
      " [434]\n",
      " [435]\n",
      " [436]\n",
      " [437]\n",
      " [438]\n",
      " [439]\n",
      " [440]\n",
      " [441]\n",
      " [442]\n",
      " [443]\n",
      " [444]\n",
      " [445]\n",
      " [446]\n",
      " [447]\n",
      " [448]\n",
      " [449]\n",
      " [450]\n",
      " [451]\n",
      " [452]\n",
      " [453]\n",
      " [454]\n",
      " [455]\n",
      " [456]\n",
      " [457]\n",
      " [458]\n",
      " [459]\n",
      " [460]\n",
      " [461]\n",
      " [462]\n",
      " [463]\n",
      " [464]\n",
      " [465]\n",
      " [466]\n",
      " [467]\n",
      " [468]\n",
      " [469]\n",
      " [470]\n",
      " [471]\n",
      " [472]\n",
      " [473]\n",
      " [474]\n",
      " [475]\n",
      " [476]\n",
      " [477]\n",
      " [478]\n",
      " [479]\n",
      " [480]\n",
      " [481]\n",
      " [482]\n",
      " [483]\n",
      " [484]\n",
      " [485]\n",
      " [486]\n",
      " [487]\n",
      " [488]\n",
      " [489]\n",
      " [490]\n",
      " [491]\n",
      " [492]\n",
      " [493]\n",
      " [494]\n",
      " [495]\n",
      " [496]\n",
      " [497]\n",
      " [498]\n",
      " [499]\n",
      " [500]\n",
      " [501]\n",
      " [502]\n",
      " [503]\n",
      " [504]\n",
      " [505]\n",
      " [506]\n",
      " [507]\n",
      " [508]\n",
      " [509]\n",
      " [510]\n",
      " [511]\n",
      " [512]\n",
      " [513]\n",
      " [514]\n",
      " [515]\n",
      " [516]\n",
      " [517]\n",
      " [518]\n",
      " [519]\n",
      " [520]\n",
      " [521]\n",
      " [522]\n",
      " [523]\n",
      " [524]\n",
      " [525]\n",
      " [526]\n",
      " [527]\n",
      " [528]\n",
      " [529]\n",
      " [530]\n",
      " [531]\n",
      " [532]\n",
      " [533]\n",
      " [534]\n",
      " [535]\n",
      " [536]\n",
      " [537]\n",
      " [538]\n",
      " [539]\n",
      " [540]\n",
      " [541]\n",
      " [542]\n",
      " [543]\n",
      " [544]\n",
      " [545]\n",
      " [546]\n",
      " [547]\n",
      " [548]\n",
      " [549]\n",
      " [550]\n",
      " [551]\n",
      " [552]\n",
      " [553]\n",
      " [554]\n",
      " [555]\n",
      " [556]\n",
      " [557]\n",
      " [558]\n",
      " [559]\n",
      " [560]\n",
      " [561]\n",
      " [562]\n",
      " [563]\n",
      " [564]\n",
      " [565]\n",
      " [566]\n",
      " [567]\n",
      " [568]\n",
      " [569]\n",
      " [570]\n",
      " [571]\n",
      " [572]\n",
      " [573]\n",
      " [574]\n",
      " [575]\n",
      " [576]\n",
      " [577]\n",
      " [578]\n",
      " [579]\n",
      " [580]\n",
      " [581]\n",
      " [582]\n",
      " [583]\n",
      " [584]\n",
      " [585]\n",
      " [586]\n",
      " [587]\n",
      " [588]\n",
      " [589]\n",
      " [590]\n",
      " [591]\n",
      " [592]\n",
      " [593]\n",
      " [594]\n",
      " [595]\n",
      " [596]\n",
      " [597]\n",
      " [598]\n",
      " [599]\n",
      " [600]\n",
      " [601]\n",
      " [602]\n",
      " [603]\n",
      " [604]\n",
      " [605]\n",
      " [606]\n",
      " [607]\n",
      " [608]\n",
      " [609]\n",
      " [610]\n",
      " [611]\n",
      " [612]\n",
      " [613]\n",
      " [614]\n",
      " [615]\n",
      " [616]\n",
      " [617]\n",
      " [618]\n",
      " [619]\n",
      " [620]\n",
      " [621]\n",
      " [622]\n",
      " [623]\n",
      " [624]\n",
      " [625]\n",
      " [626]\n",
      " [627]\n",
      " [628]\n",
      " [629]\n",
      " [630]\n",
      " [631]\n",
      " [632]\n",
      " [633]\n",
      " [634]\n",
      " [635]\n",
      " [636]\n",
      " [637]\n",
      " [638]\n",
      " [639]\n",
      " [640]\n",
      " [641]\n",
      " [642]\n",
      " [643]\n",
      " [644]\n",
      " [645]\n",
      " [646]\n",
      " [647]\n",
      " [648]\n",
      " [649]\n",
      " [650]\n",
      " [651]\n",
      " [652]\n",
      " [653]\n",
      " [654]\n",
      " [655]\n",
      " [656]\n",
      " [657]\n",
      " [658]\n",
      " [659]\n",
      " [660]\n",
      " [661]\n",
      " [662]\n",
      " [663]\n",
      " [664]\n",
      " [665]\n",
      " [666]\n",
      " [667]\n",
      " [668]\n",
      " [669]\n",
      " [670]\n",
      " [671]\n",
      " [672]\n",
      " [673]\n",
      " [674]\n",
      " [675]\n",
      " [676]\n",
      " [677]\n",
      " [678]\n",
      " [679]\n",
      " [680]\n",
      " [681]\n",
      " [682]\n",
      " [683]\n",
      " [684]\n",
      " [685]\n",
      " [686]\n",
      " [687]\n",
      " [688]\n",
      " [689]\n",
      " [690]\n",
      " [691]\n",
      " [692]\n",
      " [693]\n",
      " [694]\n",
      " [695]\n",
      " [696]\n",
      " [697]\n",
      " [698]\n",
      " [699]\n",
      " [700]\n",
      " [701]\n",
      " [702]\n",
      " [703]\n",
      " [704]\n",
      " [705]\n",
      " [706]\n",
      " [707]\n",
      " [708]\n",
      " [709]\n",
      " [710]\n",
      " [711]\n",
      " [712]\n",
      " [713]\n",
      " [714]\n",
      " [715]\n",
      " [716]\n",
      " [717]\n",
      " [718]\n",
      " [719]\n",
      " [720]\n",
      " [721]\n",
      " [722]\n",
      " [723]\n",
      " [724]\n",
      " [725]\n",
      " [726]\n",
      " [727]\n",
      " [728]\n",
      " [729]\n",
      " [730]\n",
      " [731]\n",
      " [732]\n",
      " [733]\n",
      " [734]\n",
      " [735]\n",
      " [736]\n",
      " [737]\n",
      " [738]\n",
      " [739]\n",
      " [740]\n",
      " [741]\n",
      " [742]\n",
      " [743]\n",
      " [744]\n",
      " [745]\n",
      " [746]\n",
      " [747]\n",
      " [748]\n",
      " [749]\n",
      " [750]\n",
      " [751]\n",
      " [752]\n",
      " [753]\n",
      " [754]\n",
      " [755]\n",
      " [756]\n",
      " [757]\n",
      " [758]\n",
      " [759]\n",
      " [760]\n",
      " [761]\n",
      " [762]\n",
      " [763]\n",
      " [764]\n",
      " [765]\n",
      " [766]\n",
      " [767]\n",
      " [768]\n",
      " [769]\n",
      " [770]\n",
      " [771]\n",
      " [772]\n",
      " [773]\n",
      " [774]\n",
      " [775]\n",
      " [776]\n",
      " [777]\n",
      " [778]\n",
      " [779]\n",
      " [780]\n",
      " [781]\n",
      " [782]\n",
      " [783]\n",
      " [784]\n",
      " [785]\n",
      " [786]\n",
      " [787]\n",
      " [788]\n",
      " [789]\n",
      " [790]\n",
      " [791]\n",
      " [792]\n",
      " [793]\n",
      " [794]\n",
      " [795]\n",
      " [796]\n",
      " [797]\n",
      " [798]\n",
      " [799]\n",
      " [800]\n",
      " [801]\n",
      " [802]\n",
      " [803]\n",
      " [804]\n",
      " [805]\n",
      " [806]\n",
      " [807]\n",
      " [808]\n",
      " [809]\n",
      " [810]\n",
      " [811]\n",
      " [812]\n",
      " [813]\n",
      " [814]\n",
      " [815]\n",
      " [816]\n",
      " [817]\n",
      " [818]\n",
      " [819]\n",
      " [820]\n",
      " [821]\n",
      " [822]\n",
      " [823]\n",
      " [824]\n",
      " [825]\n",
      " [826]\n",
      " [827]\n",
      " [828]\n",
      " [829]\n",
      " [830]\n",
      " [831]\n",
      " [832]\n",
      " [833]\n",
      " [834]\n",
      " [835]\n",
      " [836]\n",
      " [837]\n",
      " [838]\n",
      " [839]\n",
      " [840]\n",
      " [841]\n",
      " [842]\n",
      " [843]\n",
      " [844]\n",
      " [845]\n",
      " [846]\n",
      " [847]\n",
      " [848]\n",
      " [849]\n",
      " [850]\n",
      " [851]\n",
      " [852]\n",
      " [853]\n",
      " [854]\n",
      " [855]\n",
      " [856]\n",
      " [857]\n",
      " [858]\n",
      " [859]\n",
      " [860]\n",
      " [861]\n",
      " [862]\n",
      " [863]\n",
      " [864]\n",
      " [865]\n",
      " [866]\n",
      " [867]\n",
      " [868]\n",
      " [869]\n",
      " [870]\n",
      " [871]\n",
      " [872]\n",
      " [873]\n",
      " [874]\n",
      " [875]\n",
      " [876]\n",
      " [877]\n",
      " [878]\n",
      " [879]\n",
      " [880]\n",
      " [881]\n",
      " [882]\n",
      " [883]\n",
      " [884]\n",
      " [885]\n",
      " [886]\n",
      " [887]\n",
      " [888]\n",
      " [889]\n",
      " [890]\n",
      " [891]\n",
      " [892]\n",
      " [893]\n",
      " [894]\n",
      " [895]\n",
      " [896]\n",
      " [897]\n",
      " [898]\n",
      " [899]\n",
      " [900]\n",
      " [901]\n",
      " [902]\n",
      " [903]\n",
      " [904]\n",
      " [905]\n",
      " [906]\n",
      " [907]\n",
      " [908]\n",
      " [909]\n",
      " [910]\n",
      " [911]\n",
      " [912]\n",
      " [913]\n",
      " [914]\n",
      " [915]\n",
      " [916]\n",
      " [917]\n",
      " [918]\n",
      " [919]\n",
      " [920]\n",
      " [921]\n",
      " [922]\n",
      " [923]\n",
      " [924]\n",
      " [925]\n",
      " [926]\n",
      " [927]\n",
      " [928]\n",
      " [929]\n",
      " [930]\n",
      " [931]\n",
      " [932]\n",
      " [933]\n",
      " [934]\n",
      " [935]\n",
      " [936]\n",
      " [937]\n",
      " [938]\n",
      " [939]\n",
      " [940]\n",
      " [941]\n",
      " [942]\n",
      " [943]\n",
      " [944]\n",
      " [945]\n",
      " [946]\n",
      " [947]\n",
      " [948]\n",
      " [949]\n",
      " [950]\n",
      " [951]\n",
      " [952]\n",
      " [953]\n",
      " [954]\n",
      " [955]\n",
      " [956]\n",
      " [957]\n",
      " [958]\n",
      " [959]\n",
      " [960]\n",
      " [961]\n",
      " [962]\n",
      " [963]\n",
      " [964]\n",
      " [965]\n",
      " [966]\n",
      " [967]\n",
      " [968]\n",
      " [969]\n",
      " [970]\n",
      " [971]\n",
      " [972]\n",
      " [973]\n",
      " [974]\n",
      " [975]\n",
      " [976]\n",
      " [977]\n",
      " [978]\n",
      " [979]\n",
      " [980]\n",
      " [981]\n",
      " [982]\n",
      " [983]\n",
      " [984]\n",
      " [985]\n",
      " [986]\n",
      " [987]\n",
      " [988]\n",
      " [989]\n",
      " [990]\n",
      " [991]\n",
      " [992]\n",
      " [993]\n",
      " [994]\n",
      " [995]\n",
      " [996]\n",
      " [997]\n",
      " [998]\n",
      " [999]], shape=(1000, 1), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2593, shape=(1, 1), dtype=float32, numpy=array([[0.00475057]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staggered_train(train_sims, sims_inds, train_ders, der_inds, 10., 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(s):\n",
    "    with tf.GradientTape(persistent=True) as t:\n",
    "        x = model(s)\n",
    "    return x, t.jacobian(x, model.variables, experimental_use_pfor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradients(F, C, Cinv, dμdθ, μ_offset, dxdw, dx_mdw, dx_pdw, λ, α):\n",
    "    dCdx = get_covariance_derivative(μ_offset)    \n",
    "    dΛdx, dΛdx_m, dΛdx_p = get_log_det_fisher_derivative(F, Cinv, dμdθ, dCdx)\n",
    "    \n",
    "    dΛ_2dx, r = get_regularisation_derivative(C, Cinv, dCdx, λ, α)\n",
    "    dΛdx = tf.add(dΛ_2dx, dΛdx)\n",
    "    \n",
    "    gradients = []\n",
    "    for layer in range(len(model.variables)):\n",
    "        gradients.append(\n",
    "            tf.add(\n",
    "                tf.divide(\n",
    "                    tf.einsum(\n",
    "                        \"ij,ij...->...\", \n",
    "                        dΛdx, \n",
    "                        dxdw[layer]),\n",
    "                    ns),\n",
    "                tf.divide(\n",
    "                    tf.add(\n",
    "                        tf.einsum(\n",
    "                            \"ijk,ijk...->...\", \n",
    "                            dΛdx_m,\n",
    "                            dx_mdw[layer]),\n",
    "                        tf.einsum(\n",
    "                            \"ijk,ijk...->...\", \n",
    "                            dΛdx_p, \n",
    "                            dx_pdw[layer])),\n",
    "                    nd)))\n",
    "    opt.apply_gradients(zip(gradients, model.variables))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_iteration(s, s_m, s_p, α, λ):\n",
    "    x, dxdw = get_gradients(s)\n",
    "    x_m, dx_mdw = get_gradients(s_m)\n",
    "    x_p, dx_pdw = get_gradients(s_p)\n",
    "    \n",
    "    C, μ_offset = get_covariance(x)\n",
    "    Cinv = tf.linalg.inv(C)\n",
    "    dμdθ = get_derivative_mean(x_m, x_p)\n",
    "    \n",
    "    F = get_fisher(Cinv, dμdθ)\n",
    "        \n",
    "    r = apply_gradients(F, C, Cinv, dμdθ, μ_offset, dxdw, dx_mdw, dx_pdw, λ, α)\n",
    "    \n",
    "    return tf.linalg.det(F), tf.linalg.det(C), tf.linalg.det(Cinv), dμdθ, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate(s, s_m, s_p):\n",
    "    x = model(s)\n",
    "    x_m = model(s_m)\n",
    "    x_p = model(s_p)\n",
    "    C, _ = get_covariance(x)\n",
    "    Cinv = tf.linalg.inv(C)\n",
    "    dμdθ = get_derivative_mean(x_m, x_p)\n",
    "    F = get_fisher(Cinv, dμdθ)\n",
    "    return tf.linalg.det(F), tf.linalg.det(C), tf.linalg.det(Cinv), dμdθ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constraint_rate(ϵ, λ):\n",
    "    return np.float32(-np.log((λ - 1.) * ϵ + ϵ**2. / (1. + ϵ)) / ϵ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = np.float32(10.)\n",
    "α = get_constraint_rate(1e-2, λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_arr = []\n",
    "C_arr = []\n",
    "Cinv_arr = []\n",
    "dμdθ_arr = []\n",
    "r_arr = []\n",
    "\n",
    "test_F_arr = []\n",
    "test_C_arr = []\n",
    "test_Cinv_arr = []\n",
    "test_dμdθ_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36a3f23edd48d9ab1d1a913053261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='iterations', max=10000, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar = tqdm.tnrange(10000, desc=\"iterations\")\n",
    "for i in bar:\n",
    "    sim = next(iter(train_sims))\n",
    "    der_m, der_p = next(iter(train_ders))\n",
    "    F_temp, C_temp, Cinv_temp, dμdθ_temp, r_temp = training_iteration(sim, der_m, der_p, α, λ)\n",
    "    F_arr.append(F_temp.numpy())\n",
    "    C_arr.append(C_temp.numpy())\n",
    "    Cinv_arr.append(Cinv_temp.numpy())\n",
    "    dμdθ_arr.append(dμdθ_temp.numpy())\n",
    "    r_arr.append(r_temp.numpy())\n",
    "    sim = next(iter(test_sims))\n",
    "    der_m, der_p = next(iter(test_ders))\n",
    "    F_temp, C_temp, Cinv_temp, dμdθ_temp = evaluate(sim, der_m, der_p)\n",
    "    test_F_arr.append(F_temp.numpy())\n",
    "    test_C_arr.append(C_temp.numpy())\n",
    "    test_Cinv_arr.append(Cinv_temp.numpy())\n",
    "    test_dμdθ_arr.append(dμdθ_temp.numpy())\n",
    "    \n",
    "    bar.set_postfix(\n",
    "        F=[F_arr[-1], test_F_arr[-1]], \n",
    "        C=[C_arr[-1], test_C_arr[-1]], \n",
    "        Cinv=[Cinv_arr[-1], test_Cinv_arr[-1]], \n",
    "        dμdθ=[dμdθ_arr[-1], test_dμdθ_arr[-1]], \n",
    "        r=r_arr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F_arr, color=\"C0\", label=\"$|{\\\\bf F}|$\")\n",
    "plt.plot(test_F_arr, color=\"C1\", label=\"$|{\\\\bf F}|_{\\\\rm test}$\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$|{\\\\bf F}|$\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f74fc4f5cf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(C_arr, color=\"C0\", label=\"$|{\\\\bf C}|$\")\n",
    "plt.plot(Cinv_arr, color=\"C0\", linestyle=\"dashed\", label=\"$|{\\\\bf C}^{-1}|$\")\n",
    "plt.plot(test_C_arr, color=\"C1\", label=\"$|{\\\\bf C}|_{\\\\rm test}$\")\n",
    "plt.plot(test_Cinv_arr, color=\"C1\", linestyle=\"dashed\", label=\"$|{\\\\bf C}^{-1}|_{\\\\rm test}$\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$|{\\\\bf C}|$ and $|{\\\\bf C}^{-1}|$\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3a3bde99e2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdμdθ_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"$\\frac{\\partial\\mu}{\\partial\\theta}$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dμdθ_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"$\\left(\\frac{\\partial\\mu}{\\partial\\theta}\\right)_\\textsf{test}$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$\\frac{\\partial\\mu}{\\partial\\theta}$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "plt.plot(np.array(dμdθ_arr)[:, 0, :], color=\"C0\", label=\"$\\frac{\\partial\\mu}{\\partial\\theta}$\")\n",
    "plt.plot(np.array(test_dμdθ_arr)[:, 0, :], color=\"C1\", label=\"$\\left(\\frac{\\partial\\mu}{\\partial\\theta}\\right)_\\textsf{test}$\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$\\frac{\\partial\\mu}{\\partial\\theta}$\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_arr)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$r(\\Lambda_2)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simple_iteration(s, s_m, s_p, λ, α):\n",
    "    with tf.GradientTape(persistent=True) as taμ\n",
    "        x = model(s)\n",
    "        x_m = model(s_m)\n",
    "        x_p = model(s_p)\n",
    "    dxdw = tape.jacobian(x, model.variables)\n",
    "    dx_mdw = tape.jacobian(x_m, model.variables)\n",
    "    dx_pdw = tape.jacobian(x_p, model.variables)\n",
    "    del tape\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(x_m)\n",
    "        tape.watch(x_p)\n",
    "        C = tfp.stats.covariance(x)\n",
    "        Cinv = tf.linalg.inv(C)\n",
    "        dμdθ = tf.reduce_mean(tf.divide(tf.subtract(x_p, x_m), tf.expand_dims(tf.expand_dims(dθ, 0), 2)), axis=0)\n",
    "        F_half = tf.linalg.band_part(tf.linalg.matmul(dμdθ, tf.linalg.matmul(Cinv, dμdθ, transpose_b=True)), 0, -1)\n",
    "        F = tf.multiply(0.5, tf.add(F_half, tf.linalg.matrix_transpose(F_half)))\n",
    "        logdetF = tf.linalg.slogdet(F)\n",
    "        I = tf.eye(n_summaries, n_summaries)\n",
    "        Λ_2 = tf.add(tf.reduce_sum(tf.square(tf.subtract(C, I))), tf.reduce_sum(tf.square(tf.subtract(Cinv, I))))\n",
    "        r = tf.divide(tf.multiply(λ, Λ_2), tf.add(Λ_2, tf.exp(tf.multiply(-α, Λ_2))))\n",
    "        Λ = tf.subtract(tf.multiply(r, Λ_2), tf.multiply(logdetF[0], logdetF[1]))\n",
    "    dΛdx = tape.jacobian(Λ, x)\n",
    "    dΛdx_m = tape.jacobian(Λ, x_m)\n",
    "    dΛdx_p = tape.jacobian(Λ, x_p)\n",
    "    del tape\n",
    "    gradient = []\n",
    "    for layer in range(len(model.variables)):\n",
    "        gradient.append(\n",
    "            tf.add(\n",
    "                tf.divide(tf.einsum(\"ij...,ij->...\", dxdw[layer], dΛdx), ns),\n",
    "                tf.divide(\n",
    "                    tf.add(\n",
    "                        tf.einsum(\"ijk...,ijk->...\", dx_mdw[layer], dΛdx_m),\n",
    "                        tf.einsum(\"ijk...,ijk->...\", dx_mdw[layer], dΛdx_m)), nd)))\n",
    "    \n",
    "    opt.apply_gradients(zip(gradient, model.variables))\n",
    "    return tf.linalg.det(F), tf.linalg.det(C), tf.linalg.det(Cinv), dμdθ, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9531018\n"
     ]
    }
   ],
   "source": [
    "λ = 10.\n",
    "ϵ = 1e-2\n",
    "α = np.float32(-np.log((λ - 1.) * ϵ + ϵ**2. / (1. + ϵ)) / ϵ)\n",
    "F_arr = []\n",
    "C_arr = []\n",
    "Cinv_arr = []\n",
    "dμdθ_arr = []\n",
    "r_arr = []\n",
    "print(α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1754f6b1b94e71845769f0942c23ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='iterations', max=2500, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar = tqdm.tnrange(2500, desc=\"iterations\")\n",
    "for i in bar:\n",
    "    sim = next(iter(train_sims))\n",
    "    der_m, der_p = next(iter(train_ders))\n",
    "    F_temp, C_temp, Cinv_temp, dμdθ_temp, r_temp = simple_iteration(sim, der_m, der_p, λ, α)\n",
    "    F_arr.append(F_temp)\n",
    "    C_arr.append(C_temp)\n",
    "    Cinv_arr.append(Cinv_temp)\n",
    "    dμdθ_arr.append(dμdθ_temp)\n",
    "    r_arr.append(r_temp)\n",
    "    bar.set_postfix(F=F_arr[-1], C=C_arr[-1], Cinv=Cinv_arr[-1], dμdθ=dμdθ_arr[-1], r=r_arr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.0.0",
   "language": "python",
   "name": "tensorflow-2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
